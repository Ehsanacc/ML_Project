{"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***Simulation Question 4:***\n\nUse 80 percent of the CIFAR-10 training data to train your model.\nThis will serve as your baseline model","metadata":{"id":"51sbiNZiEQdF"}},{"cell_type":"code","source":"!git clone https://github.com/Ehsanacc/ML_Project.git","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:42:20.020662Z","iopub.execute_input":"2024-07-02T22:42:20.021043Z","iopub.status.idle":"2024-07-02T22:42:24.727086Z","shell.execute_reply.started":"2024-07-02T22:42:20.021012Z","shell.execute_reply":"2024-07-02T22:42:24.725979Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'ML_Project'...\nremote: Enumerating objects: 34, done.\u001b[K\nremote: Counting objects: 100% (34/34), done.\u001b[K\nremote: Compressing objects: 100% (33/33), done.\u001b[K\nremote: Total 34 (delta 1), reused 31 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (34/34), 33.19 MiB | 11.58 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Importing needed libraries**","metadata":{"id":"2w8sFvZeEVJ7"}},{"cell_type":"code","source":"# Importing the libraries\nfrom random import shuffle\nimport torch.optim as optim\nimport torch\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport os\nfrom torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler\nimport pickle","metadata":{"id":"bTwlwWCa5t22","execution":{"iopub.status.busy":"2024-07-02T22:44:35.967450Z","iopub.execute_input":"2024-07-02T22:44:35.967814Z","iopub.status.idle":"2024-07-02T22:44:35.974814Z","shell.execute_reply.started":"2024-07-02T22:44:35.967789Z","shell.execute_reply":"2024-07-02T22:44:35.973694Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Given base model**","metadata":{"id":"PQknFxkKEX_Q"}},{"cell_type":"code","source":"class CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(6272, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x","metadata":{"id":"aj607fEV7APj","execution":{"iopub.status.busy":"2024-07-02T20:29:13.225319Z","iopub.execute_input":"2024-07-02T20:29:13.225693Z","iopub.status.idle":"2024-07-02T20:29:13.234402Z","shell.execute_reply.started":"2024-07-02T20:29:13.225665Z","shell.execute_reply":"2024-07-02T20:29:13.233429Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Define Load data to get the needed Data**","metadata":{"id":"nuUcSMCQEdpP"}},{"cell_type":"code","source":"def get_data():\n    # CIFAR-10 dataset mean and standard deviation\n    cifar10_mean = np.array([0.49421428, 0.48513139, 0.45040909])\n    cifar10_std = np.array([0.24665252, 0.24289226, 0.26159238])\n\n    # CIFAR-10 dataset transforms\n    transform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(cifar10_mean, cifar10_std),\n    ])\n\n    # CIFAR-10 dataset transforms\n    transform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(cifar10_mean, cifar10_std),\n    ])\n\n    # Unnormalize transform for CIFAR-10 dataset\n    unnormalize_transform = transforms.Normalize(-cifar10_mean/cifar10_std, 1/cifar10_std)\n\n    # CIFAR-10 dataset loading\n    cifar10_dataset = datasets.CIFAR10(root='dataset', train=True, download=True, transform=transform_train)\n    train_dataset, val_dataset = random_split(cifar10_dataset, [45000, 5000])\n    test_dataset = datasets.CIFAR10(root='dataset', train=False, download=True, transform=transform_test)\n    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n    val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=True)\n    test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n\n    return train_loader, val_loader, test_loader\n\ntrain_loader, val_loader, test_loader = get_data()\nprint(len(train_loader))\nprint(len(val_loader))\nprint(len(test_loader))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0BJe2Su6yMu","outputId":"a402c1f8-8bc6-4742-d3c2-ca136db26847","execution":{"iopub.status.busy":"2024-07-02T22:42:24.743284Z","iopub.execute_input":"2024-07-02T22:42:24.743923Z","iopub.status.idle":"2024-07-02T22:42:26.486806Z","shell.execute_reply.started":"2024-07-02T22:42:24.743886Z","shell.execute_reply":"2024-07-02T22:42:26.485771Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n704\n79\n157\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**define model_train function**","metadata":{"id":"kWMeBvzYq-W-"}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef model_train(model, train_loader, val_loader, criterion, num_epochs, regularization_strength = None, model_name = None):\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    if regularization_strength == None:\n        regularization_strength = 0\n\n    train_loss_arr, val_loss_arr = [], []\n    train_acc_arr, val_acc_arr = [], []\n    for epoch in range(num_epochs):\n        train_loss, val_loss = .0, .0\n        train_acc, val_acc = .0, .0\n\n        model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            l2_reg = sum(torch.sum(param ** 2) for param in model.parameters())\n            loss += regularization_strength * l2_reg\n            train_loss += loss.item() * images.size(0)\n            train_acc += torch.sum(torch.max(outputs, axis=1)[1] == labels).cpu().item()\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                l2_reg = sum(torch.sum(param ** 2) for param in model.parameters())\n                loss += regularization_strength * l2_reg\n                val_loss += loss.item() * images.size(0)\n                val_acc += torch.sum(torch.max(outputs, axis=1)[1] == labels).cpu().item()\n\n        train_loss /= len(train_loader.dataset)\n        val_loss /= len(val_loader.dataset)\n        train_acc /= len(train_loader.dataset)\n        val_acc /= len(val_loader.dataset)\n\n        train_loss_arr.append(train_loss)\n        val_loss_arr.append(val_loss)\n        train_acc_arr.append(train_acc)\n        val_acc_arr.append(val_acc)\n\n        print(f\"[Epoch {epoch}]\\t\"\n            f\"[{datetime.now().strftime('%H:%M:%S')}]\\t\"\n            f\"Train Loss: {train_loss:.4f}\\t\"\n            f\"Train Accuracy: {train_acc:.2f}\\t\"\n            f\"Validation Loss: {val_loss:.4f}\\t\\t\"\n            f\"Validation Accuracy: {val_acc:.2f}\")\n        if model_name != None:\n            torch.save(model.state_dict(), f'{model_name}.pth')\n","metadata":{"id":"4ZaCr9rrAFBn","execution":{"iopub.status.busy":"2024-07-02T20:38:54.913974Z","iopub.execute_input":"2024-07-02T20:38:54.914934Z","iopub.status.idle":"2024-07-02T20:38:54.933177Z","shell.execute_reply.started":"2024-07-02T20:38:54.914897Z","shell.execute_reply":"2024-07-02T20:38:54.932010Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Train the base line model using the given data in this section**","metadata":{"id":"Nf16kIRvi6eW"}},{"cell_type":"code","source":"model = CIFAR10Classifier().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 10\n\nmodel_train(model, train_loader, val_loader, criterion, num_epochs, model_name='base_line_target_model')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ai1gpmaqSDZj","outputId":"e2b2c952-fe7b-4aaf-f2aa-125163c08f62","execution":{"iopub.status.busy":"2024-07-02T14:43:40.279697Z","iopub.execute_input":"2024-07-02T14:43:40.280058Z","iopub.status.idle":"2024-07-02T14:47:19.302933Z","shell.execute_reply.started":"2024-07-02T14:43:40.280024Z","shell.execute_reply":"2024-07-02T14:47:19.301982Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[Epoch 0]\t[14:44:02]\tTrain Loss: 1.7475\tTrain Accuracy: 0.36\tValidation Loss: 1.4524\t\tValidation Accuracy: 0.47\n[Epoch 1]\t[14:44:23]\tTrain Loss: 1.5168\tTrain Accuracy: 0.45\tValidation Loss: 1.3628\t\tValidation Accuracy: 0.51\n[Epoch 2]\t[14:44:46]\tTrain Loss: 1.4437\tTrain Accuracy: 0.48\tValidation Loss: 1.2877\t\tValidation Accuracy: 0.54\n[Epoch 3]\t[14:45:08]\tTrain Loss: 1.3904\tTrain Accuracy: 0.50\tValidation Loss: 1.2520\t\tValidation Accuracy: 0.56\n[Epoch 4]\t[14:45:29]\tTrain Loss: 1.3582\tTrain Accuracy: 0.51\tValidation Loss: 1.1963\t\tValidation Accuracy: 0.58\n[Epoch 5]\t[14:45:51]\tTrain Loss: 1.3291\tTrain Accuracy: 0.52\tValidation Loss: 1.1900\t\tValidation Accuracy: 0.58\n[Epoch 6]\t[14:46:13]\tTrain Loss: 1.3026\tTrain Accuracy: 0.53\tValidation Loss: 1.1595\t\tValidation Accuracy: 0.59\n[Epoch 7]\t[14:46:35]\tTrain Loss: 1.2855\tTrain Accuracy: 0.54\tValidation Loss: 1.1380\t\tValidation Accuracy: 0.60\n[Epoch 8]\t[14:46:57]\tTrain Loss: 1.2698\tTrain Accuracy: 0.55\tValidation Loss: 1.1033\t\tValidation Accuracy: 0.62\n[Epoch 9]\t[14:47:19]\tTrain Loss: 1.2507\tTrain Accuracy: 0.56\tValidation Loss: 1.1241\t\tValidation Accuracy: 0.61\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Check model accuracy on test_loader**","metadata":{"id":"BJQWPhiOl18k"}},{"cell_type":"code","source":"# Evaluate on the test set\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total}%\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THOcEwiiEaBv","outputId":"dfc05ce0-eae9-4a40-e50f-11933848fb4d","execution":{"iopub.status.busy":"2024-07-02T14:47:19.304411Z","iopub.execute_input":"2024-07-02T14:47:19.304796Z","iopub.status.idle":"2024-07-02T14:47:21.706852Z","shell.execute_reply.started":"2024-07-02T14:47:19.304762Z","shell.execute_reply":"2024-07-02T14:47:21.705908Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Test Accuracy: 65.38%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training phase**","metadata":{"id":"4QJI9vuDEjTs"}},{"cell_type":"markdown","source":"***Simulation Question 5:***\n\nTrain your baseline model with privacy enhancements. This is your\nmodified model. Ensure that the test accuracy difference between your baseline model and the\nmodified model is less than 15\n\n**Methods used for privacy enhancements:**\n\n    rounding output of model to 3 digits\n    Restriction of prediction vector to top 3 elements\n    Using regulatization term λ||Θ||\n\n**These methods help the model leak less information when classifying**","metadata":{"id":"CNvQO-J5XXBI"}},{"cell_type":"code","source":"class Private_CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(Private_CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Calculate the input size for the fully connected layer\n        self._to_linear = None\n        self._get_conv_output_size()\n\n        self.fc1 = nn.Linear(self._to_linear, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def _get_conv_output_size(self):\n        x = torch.rand(1, 3, 32, 32)\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = self.dropout1(x)\n        self._to_linear = x.numel()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        # only giving top k values\n        k = 2\n        topk_values, topk_indices = torch.topk(x, k, dim=1)\n        mask = torch.zeros_like(x)\n        mask.scatter_(1, topk_indices, topk_values)\n\n        # Round the values in the mask to 3 decimal places\n        rounded_mask = torch.round(mask * 100) / 100\n\n        return mask\n","metadata":{"id":"QNKly3PGXe8o","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Start training phase for the new model created as private model from the same data had from earlier**","metadata":{"id":"lyhv5JNAmMPB"}},{"cell_type":"code","source":"model = Private_CIFAR10Classifier().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nregularization_strength = 2e-3\nnum_epochs = 10\n\nmodel_train(model, train_loader, val_loader, criterion, num_epochs, regularization_strength=regularization_strength, model_name='private_target_model')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"-ARn1olN8Pb6","outputId":"728e80fd-e3bc-4041-cb27-3006a2153efc","execution":{"iopub.status.busy":"2024-07-02T14:47:21.723669Z","iopub.execute_input":"2024-07-02T14:47:21.723973Z","iopub.status.idle":"2024-07-02T14:51:01.301217Z","shell.execute_reply.started":"2024-07-02T14:47:21.723949Z","shell.execute_reply":"2024-07-02T14:51:01.300265Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[Epoch 0]\t[14:47:43]\tTrain Loss: 2.1538\tTrain Accuracy: 0.27\tValidation Loss: 2.0033\t\tValidation Accuracy: 0.38\n[Epoch 1]\t[14:48:05]\tTrain Loss: 2.0052\tTrain Accuracy: 0.35\tValidation Loss: 1.9529\t\tValidation Accuracy: 0.38\n[Epoch 2]\t[14:48:27]\tTrain Loss: 1.9572\tTrain Accuracy: 0.38\tValidation Loss: 1.8589\t\tValidation Accuracy: 0.44\n[Epoch 3]\t[14:48:49]\tTrain Loss: 1.9169\tTrain Accuracy: 0.41\tValidation Loss: 1.8330\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[14:49:11]\tTrain Loss: 1.9007\tTrain Accuracy: 0.42\tValidation Loss: 1.8208\t\tValidation Accuracy: 0.46\n[Epoch 5]\t[14:49:32]\tTrain Loss: 1.8629\tTrain Accuracy: 0.44\tValidation Loss: 1.7448\t\tValidation Accuracy: 0.49\n[Epoch 6]\t[14:49:55]\tTrain Loss: 1.8539\tTrain Accuracy: 0.44\tValidation Loss: 1.7579\t\tValidation Accuracy: 0.49\n[Epoch 7]\t[14:50:17]\tTrain Loss: 1.8316\tTrain Accuracy: 0.45\tValidation Loss: 1.7146\t\tValidation Accuracy: 0.50\n[Epoch 8]\t[14:50:39]\tTrain Loss: 1.8265\tTrain Accuracy: 0.45\tValidation Loss: 1.7326\t\tValidation Accuracy: 0.49\n[Epoch 9]\t[14:51:01]\tTrain Loss: 1.8183\tTrain Accuracy: 0.46\tValidation Loss: 1.7107\t\tValidation Accuracy: 0.51\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Check private model accuracy on the test_loader**","metadata":{"id":"5cI2rXG2mTrL"}},{"cell_type":"code","source":"# Evaluate on the test set\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total}%\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qA0qg3BjEM9Z","outputId":"a4b05532-0adc-4aee-ad11-38d49eff3964","execution":{"iopub.status.busy":"2024-07-02T14:51:01.305678Z","iopub.execute_input":"2024-07-02T14:51:01.306355Z","iopub.status.idle":"2024-07-02T14:51:03.825030Z","shell.execute_reply.started":"2024-07-02T14:51:01.306318Z","shell.execute_reply":"2024-07-02T14:51:03.823973Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Test Accuracy: 55.72%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***Simulation Question 6.***\n\n**Train two Attacker Models based on MIA techniques learned\nin Phase 0, one for the baseline model and one for the modified model. Compare the MIA\naccuracy of these two attacker models. Use 80 percent of the training data as your seen data,\nand the remaining training data along with the test data as your unseen data**","metadata":{"id":"DHQdWRjNtTY4"}},{"cell_type":"markdown","source":"**Create a shadow model to mimic the target model**","metadata":{"id":"IbcihSJgmdB1"}},{"cell_type":"code","source":"class ShadowModel(nn.Module):\n    def __init__(self):\n        super(ShadowModel, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(6272, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)","metadata":{"id":"sRQX2JwchHLA","execution":{"iopub.status.busy":"2024-07-02T22:42:28.180913Z","iopub.execute_input":"2024-07-02T22:42:28.181301Z","iopub.status.idle":"2024-07-02T22:42:28.191315Z","shell.execute_reply.started":"2024-07-02T22:42:28.181270Z","shell.execute_reply":"2024-07-02T22:42:28.190207Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**create private shadow model to mimic private target model**","metadata":{"id":"_6xrCdYTr5de"}},{"cell_type":"code","source":"class Private_ShadowModel(nn.Module):\n    def __init__(self):\n        super(Private_ShadowModel, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Calculate the input size for the fully connected layer\n        self._to_linear = None\n        self._get_conv_output_size()\n\n        self.fc1 = nn.Linear(self._to_linear, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def _get_conv_output_size(self):\n        x = torch.rand(1, 3, 32, 32)\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = self.dropout1(x)\n        self._to_linear = x.numel()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        # only giving top k values\n        k = 2\n        topk_values, topk_indices = torch.topk(x, k, dim=1)\n        mask = torch.zeros_like(x)\n        mask.scatter_(1, topk_indices, topk_values)\n\n        # Round the values in the mask to 3 decimal places\n        rounded_mask = torch.round(mask * 100) / 100\n\n        return mask\n\n\n","metadata":{"id":"oZnhEfGBZNNs","execution":{"iopub.status.busy":"2024-07-02T22:42:28.976709Z","iopub.execute_input":"2024-07-02T22:42:28.977096Z","iopub.status.idle":"2024-07-02T22:42:28.991600Z","shell.execute_reply.started":"2024-07-02T22:42:28.977065Z","shell.execute_reply":"2024-07-02T22:42:28.990288Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Train shadow model in this section for base line model**","metadata":{"id":"X9XN9GipvQXW"}},{"cell_type":"code","source":"# Train 10 different shadow models, each with its corresponding label dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nshadow_model = ShadowModel().to(device)\nnum_epochs = 5\ncriterion = nn.CrossEntropyLoss()\n\n\nprint(f\"Training shadow model ...\")\nmodel_train(shadow_model, train_loader, val_loader, criterion, num_epochs)\n\n# Save the shadow models\ntorch.save(shadow_model.state_dict(), f'shadow_model_Q6.pth')\n\n# Example: Print summary of one shadow model\nprint(shadow_model)","metadata":{"id":"LuQAHcxNtU-8","execution":{"iopub.status.busy":"2024-07-02T14:51:03.853130Z","iopub.execute_input":"2024-07-02T14:51:03.853505Z","iopub.status.idle":"2024-07-02T14:52:53.360370Z","shell.execute_reply.started":"2024-07-02T14:51:03.853470Z","shell.execute_reply":"2024-07-02T14:52:53.359443Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Training shadow model ...\n[Epoch 0]\t[14:51:26]\tTrain Loss: 2.1694\tTrain Accuracy: 0.28\tValidation Loss: 2.0998\t\tValidation Accuracy: 0.35\n[Epoch 1]\t[14:51:48]\tTrain Loss: 2.1151\tTrain Accuracy: 0.34\tValidation Loss: 2.0583\t\tValidation Accuracy: 0.40\n[Epoch 2]\t[14:52:10]\tTrain Loss: 2.0855\tTrain Accuracy: 0.37\tValidation Loss: 2.0496\t\tValidation Accuracy: 0.40\n[Epoch 3]\t[14:52:31]\tTrain Loss: 2.0655\tTrain Accuracy: 0.39\tValidation Loss: 2.0129\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[14:52:53]\tTrain Loss: 2.0519\tTrain Accuracy: 0.40\tValidation Loss: 2.0001\t\tValidation Accuracy: 0.46\nShadowModel(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout(p=0.25, inplace=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=6272, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=10, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Create datas suitable for tranining the Attacker model these datas are created by using main datas on shadow models that mimic the target model**\n\n**We get the outputs of shadow model and concat them with the true label.**\n\n**We use this pair as inputs of the attacker model and the outputs of attacker model would be in or out**","metadata":{"id":"MJTdhRXlvkQF"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to combine inputs and outputs into new dataset entries\ndef create_combined_data_of_simple_shadow(shadow_models, loaders_by_label, mode):\n    \n    shadow_model = shadow_models[0]\n    shadow_model.eval()\n\n    with torch.no_grad():\n        for images, true_outputs in loaders_by_label:\n            images = images.to(device)\n            outputs = shadow_model(images).cpu()\n            for true_output, output in zip(true_outputs, outputs):\n                true_output, output = true_output.to(device), output.to(device)\n                # Ensure true_output and output have the same number of dimensions\n                if true_output.dim() == 0:  # Handle scalar tensor case\n                    true_output = true_output.unsqueeze(0)\n                if output.dim() > 1:  # Handle higher-dimensional output tensor\n                    output = output.squeeze()  # Adjust dimensions as needed\n                combined_output = torch.cat((true_output, output), dim=0)\n                yield (combined_output, mode)\n            torch.cuda.empty_cache()\n\n# Initialize the dictionary for the shadow models\nshadow_models = {}\n\nmodel_dir = './ML_Project/'\n\n# Load shadow models\nmodel_path = os.path.join(model_dir, f'shadow_model_Q6.pth')\nshadow_model = ShadowModel().to(device)\nshadow_model.load_state_dict(torch.load(model_path, map_location=device))\nshadow_model.eval()  # Set the model to evaluation mode\nshadow_models[0] = shadow_model\n\n# Example: Print the keys of the shadow_models dictionary to verify\nprint(\"Loaded shadow models:\", shadow_models.keys())\n\n# Create seen data\ncombined_in_data1 = list(create_combined_data_of_simple_shadow(shadow_models, train_loader, 1))\ncombined_in_data2 = list(create_combined_data_of_simple_shadow(shadow_models, val_loader, 1))\ncombined_in_data = combined_in_data1 + combined_in_data2\n\n# Create unseen data\ncombined_out_data = list(create_combined_data_of_simple_shadow(shadow_models, test_loader, -1))\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab689c98-6993-4dd1-ee85-4026d15c300a","id":"r9HbnBVLvkQG","execution":{"iopub.status.busy":"2024-07-02T22:43:10.562171Z","iopub.execute_input":"2024-07-02T22:43:10.562611Z","iopub.status.idle":"2024-07-02T22:43:44.882059Z","shell.execute_reply.started":"2024-07-02T22:43:10.562579Z","shell.execute_reply":"2024-07-02T22:43:44.880962Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Loaded shadow models: dict_keys([0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Print a few data samples to see their looks and shapes**","metadata":{}},{"cell_type":"code","source":"# Example: Print first few entries of each data\nfor i in range(5):\n    input_data, label = combined_in_data[i]\n    print(f\"Input: {input_data}, Label: {label}\")\n\nfor i in range(5):\n    input_data, label = combined_out_data[i]\n    print(f\"Input: {input_data}, Label: {label}\")\n\nprint(len(combined_in_data))\nprint(len(combined_out_data))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:53:20.256030Z","iopub.execute_input":"2024-07-02T14:53:20.256387Z","iopub.status.idle":"2024-07-02T14:53:20.360112Z","shell.execute_reply.started":"2024-07-02T14:53:20.256359Z","shell.execute_reply":"2024-07-02T14:53:20.359211Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Input: tensor([9.0000e+00, 4.2485e-08, 4.7822e-08, 2.8395e-08, 2.8028e-03, 4.3366e-05,\n        2.8054e-05, 3.4315e-01, 1.3451e-01, 8.7503e-09, 5.1946e-01],\n       device='cuda:0'), Label: 1\nInput: tensor([8.0000e+00, 7.0774e-14, 1.0000e+00, 4.7829e-33, 5.0776e-30, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 9.8396e-41, 3.7384e-12, 2.8850e-06],\n       device='cuda:0'), Label: 1\nInput: tensor([6.0000e+00, 1.9649e-17, 6.3153e-13, 1.8519e-10, 3.1942e-06, 4.1173e-07,\n        3.2729e-08, 1.0000e+00, 1.6778e-07, 5.4525e-16, 1.3804e-08],\n       device='cuda:0'), Label: 1\nInput: tensor([9.0000e+00, 1.7009e-15, 1.7859e-06, 7.6702e-08, 6.3544e-05, 1.7880e-07,\n        8.0279e-06, 9.9993e-01, 1.9776e-09, 6.8186e-14, 4.0539e-08],\n       device='cuda:0'), Label: 1\nInput: tensor([2.0000e+00, 1.0000e+00, 9.0694e-16, 5.8849e-25, 7.2715e-18, 4.9520e-31,\n        1.6459e-23, 4.1276e-34, 3.3222e-24, 3.7914e-09, 1.4709e-19],\n       device='cuda:0'), Label: 1\nInput: tensor([4.0000e+00, 1.8438e-03, 3.3926e-03, 1.6443e-03, 4.1506e-01, 1.2991e-05,\n        5.1745e-01, 1.3443e-06, 5.9776e-02, 2.8087e-08, 8.1262e-04],\n       device='cuda:0'), Label: -1\nInput: tensor([4.0000e+00, 2.6370e-07, 1.7773e-09, 1.8782e-05, 6.1057e-12, 1.8857e-03,\n        4.2476e-11, 4.4047e-13, 9.9810e-01, 3.1444e-18, 1.0676e-09],\n       device='cuda:0'), Label: -1\nInput: tensor([5.0000e+00, 7.3882e-08, 9.2633e-13, 3.7857e-04, 1.7943e-06, 9.1044e-04,\n        9.2498e-05, 2.6615e-13, 9.9862e-01, 1.9710e-15, 4.4814e-11],\n       device='cuda:0'), Label: -1\nInput: tensor([2.0000e+00, 4.6353e-10, 5.1917e-10, 7.0469e-05, 2.3946e-02, 1.7676e-03,\n        9.7385e-01, 2.7273e-04, 7.3435e-06, 8.4338e-05, 3.5823e-08],\n       device='cuda:0'), Label: -1\nInput: tensor([3.0000e+00, 4.3024e-18, 2.6709e-11, 4.8469e-09, 1.8405e-07, 7.2679e-07,\n        1.2800e-09, 1.0000e+00, 3.8598e-09, 7.4103e-16, 4.3825e-10],\n       device='cuda:0'), Label: -1\n50000\n10000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Shuffle Attacker model data**","metadata":{}},{"cell_type":"code","source":"# Step 1: Combine the seen and unseen data\ndef get_shadow_datasets(combined_in_data, combined_out_data):\n    # print(len(combined_in_data))\n    # print(len(combined_out_data))\n    combined_dataset = combined_in_data + combined_out_data\n    shuffle(combined_dataset)\n    # print(len(combined_dataset))\n\n    # Step 2: Split 60,000 data into 50,000 (train/validation) and 10,000 (test)\n    train_data = combined_dataset[:50000]\n    test_data = combined_dataset[50000:]\n\n    # Create DataLoaders\n    batch_size = 64\n\n    train_shadow_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_shadow_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n    return train_shadow_loader, test_shadow_loader\n\ntrain_shadow_loader, test_shadow_loader = get_shadow_datasets(combined_in_data, combined_out_data)\n\n# Example: Print sizes to verify\n# print(f\"Total Combined Dataset Size: {len(combined_dataset)}\")\nprint(f\"Training Data Size: {len(train_shadow_loader)}\")\nprint(f\"Test Data Size: {len(test_shadow_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:43:44.884281Z","iopub.execute_input":"2024-07-02T22:43:44.884707Z","iopub.status.idle":"2024-07-02T22:43:44.956275Z","shell.execute_reply.started":"2024-07-02T22:43:44.884669Z","shell.execute_reply":"2024-07-02T22:43:44.955208Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training Data Size: 782\nTest Data Size: 157\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Prepare datas for linear regression classifier**","metadata":{}},{"cell_type":"code","source":"# Function to extract data and labels from DataLoader\ndef extract_data_and_labels(loader):\n    data = []\n    labels = []\n    for x,y in loader:\n        data.append(x.cpu().numpy())  # Assuming DataLoader returns PyTorch tensors\n        labels.append(y.cpu().numpy())\n    data = np.concatenate(data, axis=0)\n    labels = np.concatenate(labels, axis=0)\n    # normalizing\n    scaler = StandardScaler()\n    scaler.fit(data)\n    scaled_data = scaler.transform(data)\n    return scaled_data, labels\n\nX_train, y_train = extract_data_and_labels(train_shadow_loader)\nX_test, y_test = extract_data_and_labels(test_shadow_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:43:44.957750Z","iopub.execute_input":"2024-07-02T22:43:44.958157Z","iopub.status.idle":"2024-07-02T22:43:45.465505Z","shell.execute_reply.started":"2024-07-02T22:43:44.958102Z","shell.execute_reply":"2024-07-02T22:43:45.464368Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Use linear regression as attcker model (binary classifier)**","metadata":{}},{"cell_type":"code","source":"lr = LR()\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\nwith open('basic_attack_model.pkl', 'wb') as file:\n    pickle.dump(lr, file)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:45:00.286963Z","iopub.execute_input":"2024-07-02T22:45:00.287599Z","iopub.status.idle":"2024-07-02T22:45:00.358283Z","shell.execute_reply.started":"2024-07-02T22:45:00.287563Z","shell.execute_reply":"2024-07-02T22:45:00.356870Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:45:01.597326Z","iopub.execute_input":"2024-07-02T22:45:01.598247Z","iopub.status.idle":"2024-07-02T22:45:01.605625Z","shell.execute_reply.started":"2024-07-02T22:45:01.598213Z","shell.execute_reply":"2024-07-02T22:45:01.604463Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy: 83.83%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Train private shadow model in this section to mimic private model**","metadata":{}},{"cell_type":"code","source":"# Train 10 different shadow models, each with its corresponding label dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprivate_shadow_model = Private_ShadowModel().to(device)\nnum_epochs = 5\ncriterion = nn.CrossEntropyLoss()\n\n\nprint(f\"Training shadow model ...\")\nmodel_train(private_shadow_model, train_loader, val_loader, criterion, num_epochs)\n\n# Save the shadow models\ntorch.save(private_shadow_model.state_dict(), f'private_shadow_model_Q6.pth')\n\n# Example: Print summary of one shadow model\nprint(private_shadow_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:54:25.012268Z","iopub.execute_input":"2024-07-02T14:54:25.012631Z","iopub.status.idle":"2024-07-02T14:56:14.436630Z","shell.execute_reply.started":"2024-07-02T14:54:25.012604Z","shell.execute_reply":"2024-07-02T14:56:14.435732Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Training shadow model ...\n[Epoch 0]\t[14:54:46]\tTrain Loss: 2.0675\tTrain Accuracy: 0.28\tValidation Loss: 1.7886\t\tValidation Accuracy: 0.42\n[Epoch 1]\t[14:55:09]\tTrain Loss: 1.8235\tTrain Accuracy: 0.39\tValidation Loss: 1.6567\t\tValidation Accuracy: 0.48\n[Epoch 2]\t[14:55:30]\tTrain Loss: 1.7397\tTrain Accuracy: 0.42\tValidation Loss: 1.6131\t\tValidation Accuracy: 0.49\n[Epoch 3]\t[14:55:52]\tTrain Loss: 1.6844\tTrain Accuracy: 0.44\tValidation Loss: 1.5294\t\tValidation Accuracy: 0.52\n[Epoch 4]\t[14:56:14]\tTrain Loss: 1.6478\tTrain Accuracy: 0.45\tValidation Loss: 1.5000\t\tValidation Accuracy: 0.52\nPrivate_ShadowModel(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout(p=0.25, inplace=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=6272, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=10, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Create datas suitable for tranining the Attacker model these datas are created by using main datas on private shadow models that mimic the private target model**\n\n**We get the outputs of private shadow model and concat them with the true label.**\n\n**We use this pair as inputs of the attacker model and the outputs of attacker model would be in or out**","metadata":{}},{"cell_type":"code","source":"# Initialize the dictionary for the shadow models\nprivate_shadow_models = {}\n\nmodel_dir = './ML_Project/'\n\n# Load shadow models\nmodel_path = os.path.join(model_dir, f'private_shadow_model_Q6.pth')\nprivate_shadow_model = Private_ShadowModel().to(device)\nprivate_shadow_model.load_state_dict(torch.load(model_path, map_location=device))\nprivate_shadow_model.eval()  # Set the model to evaluation mode\nprivate_shadow_models[0] = private_shadow_model\n\n# Example: Print the keys of the shadow_models dictionary to verify\nprint(\"Loaded private shadow models:\", private_shadow_models.keys())\n\n# Create seen data\nprivate_combined_in_data1 = list(create_combined_data_of_simple_shadow(private_shadow_models, train_loader, 1))\nprivate_combined_in_data2 = list(create_combined_data_of_simple_shadow(private_shadow_models, val_loader, 1))\nprivate_combined_in_data = private_combined_in_data1 + private_combined_in_data2\n\n# Create unseen data\nprivate_combined_out_data = list(create_combined_data_of_simple_shadow(private_shadow_models, test_loader, -1))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:45:29.361280Z","iopub.execute_input":"2024-07-02T22:45:29.362255Z","iopub.status.idle":"2024-07-02T22:46:03.139998Z","shell.execute_reply.started":"2024-07-02T22:45:29.362219Z","shell.execute_reply":"2024-07-02T22:46:03.139165Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Loaded private shadow models: dict_keys([0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**shuffle attacker model datas**","metadata":{}},{"cell_type":"code","source":"private_train_shadow_loader, private_test_shadow_loader = get_shadow_datasets(private_combined_in_data, private_combined_out_data)\n\n# Example: Print sizes to verify\n# print(f\"Total Combined Dataset Size: {len(combined_dataset)}\")\nprint(f\"Training Data Size: {len(private_train_shadow_loader)}\")\nprint(f\"Test Data Size: {len(private_test_shadow_loader)}\")\n\nprint(f'sample train data: {private_train_shadow_loader.dataset[0]}')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:46:03.141918Z","iopub.execute_input":"2024-07-02T22:46:03.142259Z","iopub.status.idle":"2024-07-02T22:46:03.369071Z","shell.execute_reply.started":"2024-07-02T22:46:03.142232Z","shell.execute_reply":"2024-07-02T22:46:03.368014Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Training Data Size: 782\nTest Data Size: 157\nsample train data: (tensor([9.0000, 0.0000, 0.0000, 0.0000, 0.5511, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.5516], device='cuda:0'), 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**prepare data for linear regression attacker model**","metadata":{}},{"cell_type":"code","source":"# Extract data and labels from DataLoader\nprivate_X_train, private_y_train = extract_data_and_labels(private_train_shadow_loader)\nprivate_X_test, private_y_test = extract_data_and_labels(private_test_shadow_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:46:03.370571Z","iopub.execute_input":"2024-07-02T22:46:03.370970Z","iopub.status.idle":"2024-07-02T22:46:03.839253Z","shell.execute_reply.started":"2024-07-02T22:46:03.370934Z","shell.execute_reply":"2024-07-02T22:46:03.838092Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"lr = LR()\nlr.fit(private_X_train, private_y_train)\nprivate_y_pred = lr.predict(private_X_test)\nwith open('basic_private_attack_model.pkl', 'wb') as file:\n    pickle.dump(lr, file)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:46:12.913401Z","iopub.execute_input":"2024-07-02T22:46:12.914072Z","iopub.status.idle":"2024-07-02T22:46:13.053094Z","shell.execute_reply.started":"2024-07-02T22:46:12.914028Z","shell.execute_reply":"2024-07-02T22:46:13.051477Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(private_y_test, private_y_pred)\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:46:15.120130Z","iopub.execute_input":"2024-07-02T22:46:15.120598Z","iopub.status.idle":"2024-07-02T22:46:15.128312Z","shell.execute_reply.started":"2024-07-02T22:46:15.120562Z","shell.execute_reply":"2024-07-02T22:46:15.127187Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Accuracy: 82.80%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Train shadow models and save them**","metadata":{"id":"WANndorCpRhC"}},{"cell_type":"code","source":"# Train 10 different shadow models, each with its corresponding label dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nshadow_models = {}\nnum_epochs = 5\nnum_shadows = 10\ncriterion = nn.CrossEntropyLoss()\n\n\nfor s in range(num_shadows):\n    shadow_model = ShadowModel().to(device)\n    print(f\"Training shadow model number {s}...\")\n    model_train(shadow_model, train_loader, val_loader, criterion, num_epochs)\n    shadow_models[s] = shadow_model\n\n# Save the shadow models\nfor s in range(10):\n    torch.save(shadow_models[s].state_dict(), f'shadow_model_{s}.pth')\n\n# Example: Print summary of one shadow model\nprint(shadow_models[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZwxaAgzjTSn","outputId":"1d2af2fa-78f7-49ce-db29-abd5246d1c1f","execution":{"iopub.status.busy":"2024-07-02T14:57:08.931065Z","iopub.execute_input":"2024-07-02T14:57:08.931926Z","iopub.status.idle":"2024-07-02T15:15:05.287594Z","shell.execute_reply.started":"2024-07-02T14:57:08.931890Z","shell.execute_reply":"2024-07-02T15:15:05.286641Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Training shadow model number 0...\n[Epoch 0]\t[14:57:30]\tTrain Loss: 2.1786\tTrain Accuracy: 0.27\tValidation Loss: 2.0934\t\tValidation Accuracy: 0.36\n[Epoch 1]\t[14:57:52]\tTrain Loss: 2.1075\tTrain Accuracy: 0.35\tValidation Loss: 2.0593\t\tValidation Accuracy: 0.39\n[Epoch 2]\t[14:58:13]\tTrain Loss: 2.0825\tTrain Accuracy: 0.37\tValidation Loss: 2.0271\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[14:58:35]\tTrain Loss: 2.0617\tTrain Accuracy: 0.39\tValidation Loss: 2.0043\t\tValidation Accuracy: 0.45\n[Epoch 4]\t[14:58:56]\tTrain Loss: 2.0455\tTrain Accuracy: 0.41\tValidation Loss: 1.9929\t\tValidation Accuracy: 0.46\nTraining shadow model number 1...\n[Epoch 0]\t[14:59:18]\tTrain Loss: 2.1661\tTrain Accuracy: 0.28\tValidation Loss: 2.0994\t\tValidation Accuracy: 0.35\n[Epoch 1]\t[14:59:39]\tTrain Loss: 2.1027\tTrain Accuracy: 0.35\tValidation Loss: 2.0624\t\tValidation Accuracy: 0.39\n[Epoch 2]\t[15:00:01]\tTrain Loss: 2.0790\tTrain Accuracy: 0.37\tValidation Loss: 2.0295\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[15:00:22]\tTrain Loss: 2.0585\tTrain Accuracy: 0.40\tValidation Loss: 2.0135\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[15:00:44]\tTrain Loss: 2.0414\tTrain Accuracy: 0.41\tValidation Loss: 1.9882\t\tValidation Accuracy: 0.47\nTraining shadow model number 2...\n[Epoch 0]\t[15:01:05]\tTrain Loss: 2.1573\tTrain Accuracy: 0.29\tValidation Loss: 2.0857\t\tValidation Accuracy: 0.37\n[Epoch 1]\t[15:01:27]\tTrain Loss: 2.0887\tTrain Accuracy: 0.37\tValidation Loss: 2.0540\t\tValidation Accuracy: 0.40\n[Epoch 2]\t[15:01:48]\tTrain Loss: 2.0648\tTrain Accuracy: 0.39\tValidation Loss: 2.0150\t\tValidation Accuracy: 0.44\n[Epoch 3]\t[15:02:10]\tTrain Loss: 2.0465\tTrain Accuracy: 0.41\tValidation Loss: 2.0085\t\tValidation Accuracy: 0.45\n[Epoch 4]\t[15:02:31]\tTrain Loss: 2.0349\tTrain Accuracy: 0.42\tValidation Loss: 1.9946\t\tValidation Accuracy: 0.46\nTraining shadow model number 3...\n[Epoch 0]\t[15:02:53]\tTrain Loss: 2.1555\tTrain Accuracy: 0.29\tValidation Loss: 2.0732\t\tValidation Accuracy: 0.38\n[Epoch 1]\t[15:03:15]\tTrain Loss: 2.0821\tTrain Accuracy: 0.37\tValidation Loss: 2.0711\t\tValidation Accuracy: 0.38\n[Epoch 2]\t[15:03:36]\tTrain Loss: 2.0559\tTrain Accuracy: 0.40\tValidation Loss: 2.0170\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[15:03:58]\tTrain Loss: 2.0381\tTrain Accuracy: 0.42\tValidation Loss: 1.9965\t\tValidation Accuracy: 0.46\n[Epoch 4]\t[15:04:19]\tTrain Loss: 2.0204\tTrain Accuracy: 0.44\tValidation Loss: 1.9814\t\tValidation Accuracy: 0.48\nTraining shadow model number 4...\n[Epoch 0]\t[15:04:41]\tTrain Loss: 2.1603\tTrain Accuracy: 0.29\tValidation Loss: 2.1060\t\tValidation Accuracy: 0.34\n[Epoch 1]\t[15:05:03]\tTrain Loss: 2.0913\tTrain Accuracy: 0.36\tValidation Loss: 2.0421\t\tValidation Accuracy: 0.41\n[Epoch 2]\t[15:05:24]\tTrain Loss: 2.0692\tTrain Accuracy: 0.38\tValidation Loss: 2.0307\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[15:05:46]\tTrain Loss: 2.0496\tTrain Accuracy: 0.41\tValidation Loss: 2.0070\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[15:06:08]\tTrain Loss: 2.0356\tTrain Accuracy: 0.42\tValidation Loss: 1.9805\t\tValidation Accuracy: 0.48\nTraining shadow model number 5...\n[Epoch 0]\t[15:06:29]\tTrain Loss: 2.1674\tTrain Accuracy: 0.28\tValidation Loss: 2.0981\t\tValidation Accuracy: 0.36\n[Epoch 1]\t[15:06:51]\tTrain Loss: 2.1022\tTrain Accuracy: 0.35\tValidation Loss: 2.0508\t\tValidation Accuracy: 0.40\n[Epoch 2]\t[15:07:12]\tTrain Loss: 2.0764\tTrain Accuracy: 0.38\tValidation Loss: 2.0329\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[15:07:34]\tTrain Loss: 2.0561\tTrain Accuracy: 0.40\tValidation Loss: 2.0161\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[15:07:55]\tTrain Loss: 2.0432\tTrain Accuracy: 0.41\tValidation Loss: 1.9988\t\tValidation Accuracy: 0.46\nTraining shadow model number 6...\n[Epoch 0]\t[15:08:17]\tTrain Loss: 2.1597\tTrain Accuracy: 0.29\tValidation Loss: 2.0712\t\tValidation Accuracy: 0.38\n[Epoch 1]\t[15:08:38]\tTrain Loss: 2.0865\tTrain Accuracy: 0.37\tValidation Loss: 2.0306\t\tValidation Accuracy: 0.42\n[Epoch 2]\t[15:09:00]\tTrain Loss: 2.0580\tTrain Accuracy: 0.40\tValidation Loss: 2.0123\t\tValidation Accuracy: 0.44\n[Epoch 3]\t[15:09:21]\tTrain Loss: 2.0379\tTrain Accuracy: 0.42\tValidation Loss: 1.9866\t\tValidation Accuracy: 0.47\n[Epoch 4]\t[15:09:43]\tTrain Loss: 2.0244\tTrain Accuracy: 0.43\tValidation Loss: 1.9722\t\tValidation Accuracy: 0.48\nTraining shadow model number 7...\n[Epoch 0]\t[15:10:04]\tTrain Loss: 2.1706\tTrain Accuracy: 0.28\tValidation Loss: 2.0993\t\tValidation Accuracy: 0.36\n[Epoch 1]\t[15:10:26]\tTrain Loss: 2.1048\tTrain Accuracy: 0.35\tValidation Loss: 2.0592\t\tValidation Accuracy: 0.40\n[Epoch 2]\t[15:10:48]\tTrain Loss: 2.0786\tTrain Accuracy: 0.38\tValidation Loss: 2.0313\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[15:11:09]\tTrain Loss: 2.0607\tTrain Accuracy: 0.39\tValidation Loss: 2.0034\t\tValidation Accuracy: 0.45\n[Epoch 4]\t[15:11:31]\tTrain Loss: 2.0425\tTrain Accuracy: 0.41\tValidation Loss: 1.9897\t\tValidation Accuracy: 0.47\nTraining shadow model number 8...\n[Epoch 0]\t[15:11:52]\tTrain Loss: 2.1721\tTrain Accuracy: 0.27\tValidation Loss: 2.0916\t\tValidation Accuracy: 0.36\n[Epoch 1]\t[15:12:13]\tTrain Loss: 2.1034\tTrain Accuracy: 0.35\tValidation Loss: 2.0553\t\tValidation Accuracy: 0.40\n[Epoch 2]\t[15:12:35]\tTrain Loss: 2.0794\tTrain Accuracy: 0.38\tValidation Loss: 2.0287\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[15:12:56]\tTrain Loss: 2.0627\tTrain Accuracy: 0.39\tValidation Loss: 2.0218\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[15:13:17]\tTrain Loss: 2.0467\tTrain Accuracy: 0.41\tValidation Loss: 2.0221\t\tValidation Accuracy: 0.43\nTraining shadow model number 9...\n[Epoch 0]\t[15:13:39]\tTrain Loss: 2.1656\tTrain Accuracy: 0.28\tValidation Loss: 2.0992\t\tValidation Accuracy: 0.35\n[Epoch 1]\t[15:14:00]\tTrain Loss: 2.1067\tTrain Accuracy: 0.35\tValidation Loss: 2.0569\t\tValidation Accuracy: 0.40\n[Epoch 2]\t[15:14:22]\tTrain Loss: 2.0809\tTrain Accuracy: 0.37\tValidation Loss: 2.0404\t\tValidation Accuracy: 0.42\n[Epoch 3]\t[15:14:43]\tTrain Loss: 2.0630\tTrain Accuracy: 0.39\tValidation Loss: 2.0177\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[15:15:05]\tTrain Loss: 2.0491\tTrain Accuracy: 0.41\tValidation Loss: 2.0038\t\tValidation Accuracy: 0.45\nShadowModel(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout(p=0.25, inplace=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=6272, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=10, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Create datas suitable for tranining the Attacker model these datas are created by using main datas on shadow models that mimic the target model**","metadata":{"id":"0zjl9I_4yoT5"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to combine inputs and outputs into new dataset entries\ndef create_combined_data(shadow_models, loader, mode):\n    for s in range(len(shadow_models)):\n        shadow_model = shadow_models[s]\n        shadow_model.eval()\n\n        with torch.no_grad():\n            for images, true_outputs in loader:\n                images = images.to(device) \n                outputs = shadow_model(images).cpu()\n                for true_output, output in zip(true_outputs, outputs):\n                    true_output, output = true_output.to(device), output.to(device)\n                    # Ensure true_output and output have the same number of dimensions\n                    if true_output.dim() == 0:  # Handle scalar tensor case\n                        true_output = true_output.unsqueeze(0)\n                    if output.dim() > 1:  # Handle higher-dimensional output tensor\n                        output = output.squeeze()  # Adjust dimensions as needed\n                    combined_output = torch.cat((true_output, output), dim=0)\n                    yield (combined_output, mode)\n                torch.cuda.empty_cache()\n\n# Initialize the dictionary for the shadow models\nshadow_models = {}\nnum_shodow_models = 10\nmodel_dir = './ML_Project/shadow_models/'\n\n# Load shadow models\nfor i in range(num_shodow_models):\n    model_path = os.path.join(model_dir, f'shadow_model_{i}.pth')\n    shadow_model = ShadowModel().to(device)\n    shadow_model.load_state_dict(torch.load(model_path, map_location=device))\n    shadow_model.eval()  # Set the model to evaluation mode\n    shadow_models[i] = shadow_model\n\n# Example: Print the keys of the shadow_models dictionary to verify\nprint(\"Loaded shadow models:\", shadow_models.keys())\n\n# Create seen data\ncombined_in_data1 = list(create_combined_data(shadow_models, train_loader, 1))\ncombined_in_data2 = list(create_combined_data(shadow_models, val_loader, 1))\ncombined_in_data = combined_in_data1 + combined_in_data2\n\n# Create unseen data\ncombined_out_data = list(create_combined_data(shadow_models, test_loader, -1))\n\n","metadata":{"id":"0CzpW3ZWNh-Z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab689c98-6993-4dd1-ee85-4026d15c300a","execution":{"iopub.status.busy":"2024-07-02T22:47:54.490904Z","iopub.execute_input":"2024-07-02T22:47:54.491559Z","iopub.status.idle":"2024-07-02T22:53:18.891774Z","shell.execute_reply.started":"2024-07-02T22:47:54.491528Z","shell.execute_reply":"2024-07-02T22:53:18.890807Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Loaded shadow models: dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Print a few sample datas to see their looks**","metadata":{"id":"dFjxC0mUyza6"}},{"cell_type":"code","source":"# Example: Print first few entries of each data\nfor i in range(5):\n    input_data, label = combined_in_data[i]\n    print(f\"Input: {input_data}, Label: {label}\")\n\nfor i in range(5):\n    input_data, label = combined_out_data[i]\n    print(f\"Input: {input_data}, Label: {label}\")\n\nprint(len(combined_in_data))\nprint(len(combined_out_data))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LThs6kW9BJbk","outputId":"8205338c-a3f2-4e94-f7fa-5ab3c4d52c5d","execution":{"iopub.status.busy":"2024-07-02T15:21:10.931148Z","iopub.execute_input":"2024-07-02T15:21:10.931535Z","iopub.status.idle":"2024-07-02T15:21:10.947990Z","shell.execute_reply.started":"2024-07-02T15:21:10.931505Z","shell.execute_reply":"2024-07-02T15:21:10.947006Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Input: tensor([1.0000e+00, 5.7000e-12, 9.9997e-01, 1.3633e-10, 1.3945e-11, 1.7988e-17,\n        2.9970e-17, 1.3210e-15, 8.0076e-18, 7.0411e-07, 2.4437e-05],\n       device='cuda:0'), Label: 1\nInput: tensor([9.0000e+00, 7.3594e-11, 7.5609e-06, 5.1034e-12, 6.0145e-07, 1.1050e-13,\n        7.1933e-10, 9.3258e-08, 3.8030e-05, 1.1856e-08, 9.9995e-01],\n       device='cuda:0'), Label: 1\nInput: tensor([7.0000e+00, 7.6815e-04, 1.5189e-02, 6.7124e-02, 5.4361e-01, 5.5002e-02,\n        5.8696e-02, 2.2467e-01, 2.0750e-02, 1.8769e-03, 1.2312e-02],\n       device='cuda:0'), Label: 1\nInput: tensor([8.0000e+00, 8.4091e-01, 1.1952e-02, 9.3768e-05, 8.8409e-02, 1.0779e-06,\n        5.8708e-04, 4.3390e-08, 2.5747e-05, 5.2637e-02, 5.3849e-03],\n       device='cuda:0'), Label: 1\nInput: tensor([0.0000, 0.1549, 0.0007, 0.0689, 0.0381, 0.1775, 0.0926, 0.0026, 0.4606,\n        0.0006, 0.0035], device='cuda:0'), Label: 1\nInput: tensor([7.0000e+00, 1.5993e-08, 3.2558e-14, 1.3142e-10, 6.8014e-15, 4.1251e-10,\n        4.0823e-10, 1.8324e-13, 1.0000e+00, 2.0293e-25, 1.6176e-13],\n       device='cuda:0'), Label: -1\nInput: tensor([3.0000e+00, 6.6074e-23, 3.2279e-09, 2.8033e-15, 2.9297e-06, 2.2770e-14,\n        1.1411e-13, 1.0000e+00, 1.0047e-11, 5.5602e-19, 5.8257e-08],\n       device='cuda:0'), Label: -1\nInput: tensor([2.0000e+00, 8.4727e-09, 3.1985e-05, 5.3729e-05, 1.1257e-04, 3.9741e-04,\n        5.6721e-05, 9.9350e-01, 5.7841e-03, 4.8504e-10, 5.8702e-05],\n       device='cuda:0'), Label: -1\nInput: tensor([2.0000e+00, 9.1457e-14, 2.1005e-20, 2.2176e-06, 7.0072e-04, 2.4825e-09,\n        9.9930e-01, 1.8728e-16, 1.1753e-09, 4.9784e-16, 6.7300e-21],\n       device='cuda:0'), Label: -1\nInput: tensor([3.0000e+00, 3.1744e-04, 2.0695e-10, 2.2470e-05, 8.1231e-01, 3.4953e-09,\n        1.8735e-01, 4.6857e-13, 2.1038e-06, 7.5421e-07, 1.3674e-09],\n       device='cuda:0'), Label: -1\n500000\n100000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**create final dataset for the attacker model**","metadata":{"id":"n_EdcveEuO9m"}},{"cell_type":"code","source":"# Step 1: Combine the seen and unseen data\ndef get_shadow_datasets(combined_in_data, combined_out_data):\n    # print(len(combined_in_data))\n    # print(len(combined_out_data))\n    combined_dataset = combined_in_data + combined_out_data\n    shuffle(combined_dataset)\n#     print(len(combined_dataset))\n\n    # Create DataLoaders\n    batch_size = 64\n\n    train_shadow_loader = DataLoader(combined_dataset[0:48000], batch_size=batch_size, shuffle=True)\n    test_shadow_loader = DataLoader(combined_dataset[48000:60000], batch_size=batch_size, shuffle=True)\n#     print(len(train_shadow_loader))\n#     print(test_shadow_loader.dataset[0])\n    return train_shadow_loader, test_shadow_loader\n\ntrain_shadow_loader, test_shadow_loader = get_shadow_datasets(combined_in_data, combined_out_data)\n\n# Example: Print sizes to verify\n# print(f\"Total Combined Dataset Size: {len(combined_dataset)}\")\nprint(f\"Training Data Size: {len(train_shadow_loader)}\")\nprint(f\"Test Data Size: {len(test_shadow_loader)}\")","metadata":{"id":"NAv6q2zZY-ta","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb973a13-4370-45f8-fd0b-dbca38576172","execution":{"iopub.status.busy":"2024-07-02T22:53:18.893675Z","iopub.execute_input":"2024-07-02T22:53:18.893998Z","iopub.status.idle":"2024-07-02T22:53:19.732066Z","shell.execute_reply.started":"2024-07-02T22:53:18.893972Z","shell.execute_reply":"2024-07-02T22:53:19.730963Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Training Data Size: 750\nTest Data Size: 188\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**see a sample data**","metadata":{"id":"5Bn6zuWeuiS4"}},{"cell_type":"code","source":"print(f'sample train data: {train_shadow_loader.dataset[0]}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLrRv2mPG7m0","outputId":"6a58f434-df47-4ae3-da07-bc15bbd10a17","execution":{"iopub.status.busy":"2024-07-02T17:11:58.430147Z","iopub.execute_input":"2024-07-02T17:11:58.430976Z","iopub.status.idle":"2024-07-02T17:11:58.437227Z","shell.execute_reply.started":"2024-07-02T17:11:58.430946Z","shell.execute_reply":"2024-07-02T17:11:58.436335Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"sample train data: (tensor([9.0000e+00, 6.6451e-08, 1.0000e+00, 7.0550e-14, 1.4830e-12, 1.4231e-24,\n        1.7990e-18, 2.1469e-22, 1.5359e-19, 1.2996e-09, 1.1756e-06],\n       device='cuda:0'), 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**preprocess data to train linear regression model as attacker model**","metadata":{"id":"3Hwngm2uvV8r"}},{"cell_type":"code","source":"def extract_data_and_labels(loader):\n    data = []\n    labels = []\n    for x,y in loader:\n#         print(x.cpu().numpy().shape)\n        data.append(x.cpu().numpy())  # Assuming DataLoader returns PyTorch tensors\n        labels.append(y.cpu().numpy())\n    data = np.concatenate(data, axis=0)\n    labels = np.concatenate(labels, axis=0)\n    # normalizing\n    scaler = StandardScaler()\n    scaler.fit(data)\n    scaled_data = scaler.transform(data)\n    return scaled_data, labels\n\nX_train, y_train = extract_data_and_labels(train_shadow_loader)\nX_test, y_test = extract_data_and_labels(test_shadow_loader)\nprint(X_train.shape)\nprint(y_train.shape)","metadata":{"id":"IjyJbzhMWaxL","execution":{"iopub.status.busy":"2024-07-02T22:53:19.733959Z","iopub.execute_input":"2024-07-02T22:53:19.734297Z","iopub.status.idle":"2024-07-02T22:53:20.041861Z","shell.execute_reply.started":"2024-07-02T22:53:19.734268Z","shell.execute_reply":"2024-07-02T22:53:20.040829Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(48000, 11)\n(48000,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**simple logistic regression model as attacker**","metadata":{"id":"kEQ62GPDvliJ"}},{"cell_type":"code","source":"lr = LR()\nlr.fit(X_train, y_train)\n# Evaluate the model\ny_pred = lr.predict(X_test)\nwith open('advanced_attack_model.pkl', 'wb') as file:\n    pickle.dump(lr, file)","metadata":{"id":"xQxOdjNCXPWr","execution":{"iopub.status.busy":"2024-07-02T22:53:20.044555Z","iopub.execute_input":"2024-07-02T22:53:20.044955Z","iopub.status.idle":"2024-07-02T22:53:20.116077Z","shell.execute_reply.started":"2024-07-02T22:53:20.044918Z","shell.execute_reply":"2024-07-02T22:53:20.114339Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**check attacker model accuracy**","metadata":{"id":"tp_4LwlxvvAF"}},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3K6_-ltvuJB","outputId":"1dd213b1-135a-44fe-fb1a-a8df8ff1572f","execution":{"iopub.status.busy":"2024-07-02T22:53:20.118372Z","iopub.execute_input":"2024-07-02T22:53:20.119824Z","iopub.status.idle":"2024-07-02T22:53:20.128637Z","shell.execute_reply.started":"2024-07-02T22:53:20.119763Z","shell.execute_reply":"2024-07-02T22:53:20.127266Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Accuracy: 83.48%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**create private shadow models to mimic private model**","metadata":{"id":"oj2GkIMBvzzq"}},{"cell_type":"code","source":"# Train 10 different shadow models, each with its corresponding label dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprivate_shadow_models = {}\nregularization_strength = 2e-3\nnum_epochs = 5\ncriterion = nn.CrossEntropyLoss()\n\nfor label in range(10):\n    private_shadow_model = Private_ShadowModel().to(device)\n    print(f\"Training shadow model for label {label}...\")\n    model_train(private_shadow_model, train_loader, val_loader, criterion, num_epochs, regularization_strength)\n    private_shadow_models[label] = private_shadow_model\n\n# Save the shadow models\nfor label in range(10):\n    torch.save(private_shadow_models[label].state_dict(), f'private_shadow_model_{label}.pth')\n\n# Example: Print summary of one shadow model\nprint(private_shadow_models[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8U0zm7YWZwcc","outputId":"52aa4a06-6fbd-48ab-b50e-620a26dcffcf","execution":{"iopub.status.busy":"2024-07-02T16:09:28.545682Z","iopub.execute_input":"2024-07-02T16:09:28.546039Z","iopub.status.idle":"2024-07-02T16:27:38.584555Z","shell.execute_reply.started":"2024-07-02T16:09:28.546011Z","shell.execute_reply":"2024-07-02T16:27:38.583652Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Training shadow model for label 0...\n[Epoch 0]\t[16:09:50]\tTrain Loss: 2.1372\tTrain Accuracy: 0.29\tValidation Loss: 1.9093\t\tValidation Accuracy: 0.41\n[Epoch 1]\t[16:10:12]\tTrain Loss: 1.9859\tTrain Accuracy: 0.37\tValidation Loss: 1.8262\t\tValidation Accuracy: 0.44\n[Epoch 2]\t[16:10:34]\tTrain Loss: 1.9207\tTrain Accuracy: 0.41\tValidation Loss: 1.7706\t\tValidation Accuracy: 0.47\n[Epoch 3]\t[16:10:55]\tTrain Loss: 1.8877\tTrain Accuracy: 0.43\tValidation Loss: 1.7127\t\tValidation Accuracy: 0.49\n[Epoch 4]\t[16:11:17]\tTrain Loss: 1.8601\tTrain Accuracy: 0.44\tValidation Loss: 1.6945\t\tValidation Accuracy: 0.50\nTraining shadow model for label 1...\n[Epoch 0]\t[16:11:39]\tTrain Loss: 2.1989\tTrain Accuracy: 0.23\tValidation Loss: 2.0859\t\tValidation Accuracy: 0.32\n[Epoch 1]\t[16:12:01]\tTrain Loss: 2.0627\tTrain Accuracy: 0.33\tValidation Loss: 1.8970\t\tValidation Accuracy: 0.42\n[Epoch 2]\t[16:12:23]\tTrain Loss: 1.9674\tTrain Accuracy: 0.38\tValidation Loss: 1.8254\t\tValidation Accuracy: 0.45\n[Epoch 3]\t[16:12:44]\tTrain Loss: 1.9084\tTrain Accuracy: 0.41\tValidation Loss: 1.7758\t\tValidation Accuracy: 0.48\n[Epoch 4]\t[16:13:06]\tTrain Loss: 1.8745\tTrain Accuracy: 0.43\tValidation Loss: 1.7083\t\tValidation Accuracy: 0.50\nTraining shadow model for label 2...\n[Epoch 0]\t[16:13:28]\tTrain Loss: 2.1786\tTrain Accuracy: 0.25\tValidation Loss: 2.0067\t\tValidation Accuracy: 0.36\n[Epoch 1]\t[16:13:50]\tTrain Loss: 2.0369\tTrain Accuracy: 0.33\tValidation Loss: 1.8584\t\tValidation Accuracy: 0.41\n[Epoch 2]\t[16:14:11]\tTrain Loss: 1.9654\tTrain Accuracy: 0.38\tValidation Loss: 1.8268\t\tValidation Accuracy: 0.44\n[Epoch 3]\t[16:14:33]\tTrain Loss: 1.9321\tTrain Accuracy: 0.40\tValidation Loss: 1.7750\t\tValidation Accuracy: 0.46\n[Epoch 4]\t[16:14:55]\tTrain Loss: 1.8941\tTrain Accuracy: 0.42\tValidation Loss: 1.6903\t\tValidation Accuracy: 0.49\nTraining shadow model for label 3...\n[Epoch 0]\t[16:15:17]\tTrain Loss: 2.1837\tTrain Accuracy: 0.24\tValidation Loss: 1.9609\t\tValidation Accuracy: 0.38\n[Epoch 1]\t[16:15:38]\tTrain Loss: 2.0241\tTrain Accuracy: 0.35\tValidation Loss: 1.8912\t\tValidation Accuracy: 0.41\n[Epoch 2]\t[16:16:00]\tTrain Loss: 1.9459\tTrain Accuracy: 0.39\tValidation Loss: 1.7790\t\tValidation Accuracy: 0.46\n[Epoch 3]\t[16:16:22]\tTrain Loss: 1.9075\tTrain Accuracy: 0.41\tValidation Loss: 1.7438\t\tValidation Accuracy: 0.48\n[Epoch 4]\t[16:16:44]\tTrain Loss: 1.8833\tTrain Accuracy: 0.42\tValidation Loss: 1.7376\t\tValidation Accuracy: 0.48\nTraining shadow model for label 4...\n[Epoch 0]\t[16:17:06]\tTrain Loss: 2.1432\tTrain Accuracy: 0.28\tValidation Loss: 1.9221\t\tValidation Accuracy: 0.39\n[Epoch 1]\t[16:17:27]\tTrain Loss: 1.9894\tTrain Accuracy: 0.36\tValidation Loss: 1.8075\t\tValidation Accuracy: 0.44\n[Epoch 2]\t[16:17:49]\tTrain Loss: 1.9207\tTrain Accuracy: 0.40\tValidation Loss: 1.7316\t\tValidation Accuracy: 0.49\n[Epoch 3]\t[16:18:11]\tTrain Loss: 1.8790\tTrain Accuracy: 0.42\tValidation Loss: 1.7218\t\tValidation Accuracy: 0.48\n[Epoch 4]\t[16:18:33]\tTrain Loss: 1.8531\tTrain Accuracy: 0.44\tValidation Loss: 1.7076\t\tValidation Accuracy: 0.50\nTraining shadow model for label 5...\n[Epoch 0]\t[16:18:55]\tTrain Loss: 2.1931\tTrain Accuracy: 0.24\tValidation Loss: 2.0049\t\tValidation Accuracy: 0.36\n[Epoch 1]\t[16:19:17]\tTrain Loss: 2.0276\tTrain Accuracy: 0.34\tValidation Loss: 1.8440\t\tValidation Accuracy: 0.43\n[Epoch 2]\t[16:19:38]\tTrain Loss: 1.9561\tTrain Accuracy: 0.38\tValidation Loss: 1.7668\t\tValidation Accuracy: 0.45\n[Epoch 3]\t[16:20:00]\tTrain Loss: 1.9048\tTrain Accuracy: 0.40\tValidation Loss: 1.7617\t\tValidation Accuracy: 0.47\n[Epoch 4]\t[16:20:22]\tTrain Loss: 1.8740\tTrain Accuracy: 0.42\tValidation Loss: 1.7026\t\tValidation Accuracy: 0.51\nTraining shadow model for label 6...\n[Epoch 0]\t[16:20:44]\tTrain Loss: 2.1431\tTrain Accuracy: 0.28\tValidation Loss: 1.9864\t\tValidation Accuracy: 0.37\n[Epoch 1]\t[16:21:06]\tTrain Loss: 2.0197\tTrain Accuracy: 0.36\tValidation Loss: 1.8945\t\tValidation Accuracy: 0.44\n[Epoch 2]\t[16:21:27]\tTrain Loss: 1.9529\tTrain Accuracy: 0.40\tValidation Loss: 1.7807\t\tValidation Accuracy: 0.46\n[Epoch 3]\t[16:21:49]\tTrain Loss: 1.8992\tTrain Accuracy: 0.42\tValidation Loss: 1.7481\t\tValidation Accuracy: 0.48\n[Epoch 4]\t[16:22:11]\tTrain Loss: 1.8673\tTrain Accuracy: 0.43\tValidation Loss: 1.7341\t\tValidation Accuracy: 0.52\nTraining shadow model for label 7...\n[Epoch 0]\t[16:22:33]\tTrain Loss: 2.1520\tTrain Accuracy: 0.27\tValidation Loss: 1.9443\t\tValidation Accuracy: 0.37\n[Epoch 1]\t[16:22:55]\tTrain Loss: 2.0081\tTrain Accuracy: 0.35\tValidation Loss: 1.8844\t\tValidation Accuracy: 0.39\n[Epoch 2]\t[16:23:17]\tTrain Loss: 1.9596\tTrain Accuracy: 0.37\tValidation Loss: 1.8433\t\tValidation Accuracy: 0.42\n[Epoch 3]\t[16:23:38]\tTrain Loss: 1.9296\tTrain Accuracy: 0.39\tValidation Loss: 1.8060\t\tValidation Accuracy: 0.44\n[Epoch 4]\t[16:24:00]\tTrain Loss: 1.9191\tTrain Accuracy: 0.39\tValidation Loss: 1.7793\t\tValidation Accuracy: 0.45\nTraining shadow model for label 8...\n[Epoch 0]\t[16:24:22]\tTrain Loss: 2.1545\tTrain Accuracy: 0.26\tValidation Loss: 2.0240\t\tValidation Accuracy: 0.32\n[Epoch 1]\t[16:24:44]\tTrain Loss: 2.0493\tTrain Accuracy: 0.32\tValidation Loss: 1.9292\t\tValidation Accuracy: 0.39\n[Epoch 2]\t[16:25:06]\tTrain Loss: 1.9973\tTrain Accuracy: 0.35\tValidation Loss: 1.8964\t\tValidation Accuracy: 0.40\n[Epoch 3]\t[16:25:27]\tTrain Loss: 1.9826\tTrain Accuracy: 0.37\tValidation Loss: 1.8665\t\tValidation Accuracy: 0.41\n[Epoch 4]\t[16:25:49]\tTrain Loss: 1.9543\tTrain Accuracy: 0.37\tValidation Loss: 1.8253\t\tValidation Accuracy: 0.42\nTraining shadow model for label 9...\n[Epoch 0]\t[16:26:11]\tTrain Loss: 2.1566\tTrain Accuracy: 0.27\tValidation Loss: 1.9817\t\tValidation Accuracy: 0.35\n[Epoch 1]\t[16:26:33]\tTrain Loss: 2.0362\tTrain Accuracy: 0.34\tValidation Loss: 1.9336\t\tValidation Accuracy: 0.40\n[Epoch 2]\t[16:26:54]\tTrain Loss: 1.9821\tTrain Accuracy: 0.37\tValidation Loss: 1.8442\t\tValidation Accuracy: 0.43\n[Epoch 3]\t[16:27:16]\tTrain Loss: 1.9381\tTrain Accuracy: 0.40\tValidation Loss: 1.7855\t\tValidation Accuracy: 0.47\n[Epoch 4]\t[16:27:38]\tTrain Loss: 1.9015\tTrain Accuracy: 0.42\tValidation Loss: 1.7814\t\tValidation Accuracy: 0.47\nPrivate_ShadowModel(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout(p=0.25, inplace=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=6272, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=10, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**read saved private shadow models and prepare data for attacker model**","metadata":{"id":"m7Vav6O2v66y"}},{"cell_type":"code","source":"# Initialize the dictionary for the shadow models\nprivate_shadow_models = {}\n\nmodel_dir = './ML_Project/private_shadow_models/'\n\n# Load shadow models\nfor i in range(10):\n    model_path = os.path.join(model_dir, f'private_shadow_model_{i}.pth')\n    private_shadow_model = Private_ShadowModel().to(device)\n    private_shadow_model.load_state_dict(torch.load(model_path, map_location=device))\n    private_shadow_model.eval()  # Set the model to evaluation mode\n    private_shadow_models[i] = private_shadow_model\n\n# Example: Print the keys of the shadow_models dictionary to verify\nprint(\"Loaded private shadow models:\", private_shadow_models.keys())\n\n# Create seen data\nprivate_combined_in_data1 = list(create_combined_data(private_shadow_models, train_loader, 1))\nprivate_combined_in_data2 = list(create_combined_data(private_shadow_models, val_loader, 1))\nprivate_combined_in_data = private_combined_in_data1 + private_combined_in_data2\n\n# Create unseen data\nprivate_combined_out_data = list(create_combined_data(private_shadow_models, test_loader, -1))\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqBP12Q7aW99","outputId":"a94b410d-a5fe-44e9-ecd1-0a7a7bf620b9","execution":{"iopub.status.busy":"2024-07-02T22:54:22.201306Z","iopub.execute_input":"2024-07-02T22:54:22.202607Z","iopub.status.idle":"2024-07-02T23:00:05.507184Z","shell.execute_reply.started":"2024-07-02T22:54:22.202557Z","shell.execute_reply":"2024-07-02T23:00:05.506015Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Loaded private shadow models: dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**see samples of final data**","metadata":{"id":"wp1c80Qnv_k6"}},{"cell_type":"code","source":"# Example: Print first few entries of each data\nfor i in range(5):\n    input_data, label = private_combined_in_data[i]\n    print(f\"Input: {input_data}, Label: {label}\")\n\nfor i in range(5):\n    input_data, label = private_combined_out_data[i]\n    print(f\"Input: {input_data}, Label: {label}\")\n\nprint(len(private_combined_in_data))\nprint(len(private_combined_out_data))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ie4fH-1SbT0S","outputId":"d1f0e7da-bf9c-4ab4-dd5e-e4c1258c5755","execution":{"iopub.status.busy":"2024-07-02T17:30:38.127172Z","iopub.execute_input":"2024-07-02T17:30:38.127503Z","iopub.status.idle":"2024-07-02T17:30:38.143705Z","shell.execute_reply.started":"2024-07-02T17:30:38.127476Z","shell.execute_reply":"2024-07-02T17:30:38.142608Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"Input: tensor([9.0000, 0.0000, 2.4825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 4.0832], device='cuda:0'), Label: 1\nInput: tensor([2.0000, 0.0000, 0.0000, 0.0000, 1.0437, 0.0000, 1.2690, 0.0000, 0.0000,\n        0.0000, 0.0000], device='cuda:0'), Label: 1\nInput: tensor([7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0462,\n        0.0000, 1.6653], device='cuda:0'), Label: 1\nInput: tensor([7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7650, 0.8787, 0.0000, 0.0000,\n        0.0000, 0.0000], device='cuda:0'), Label: 1\nInput: tensor([5.0000, 3.9677, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        2.5984, 0.0000], device='cuda:0'), Label: 1\nInput: tensor([4.0000, 0.0000, 0.0000, 0.0000, 1.4472, 0.0000, 1.6715, 0.0000, 0.0000,\n        0.0000, 0.0000], device='cuda:0'), Label: -1\nInput: tensor([5.0000, 0.0000, 0.0000, 0.0000, 1.4383, 0.0000, 2.0437, 0.0000, 0.0000,\n        0.0000, 0.0000], device='cuda:0'), Label: -1\nInput: tensor([9.0000, 0.0000, 3.6093, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 3.6882], device='cuda:0'), Label: -1\nInput: tensor([3.0000, 0.0000, 0.0000, 0.0000, 1.9907, 0.0000, 2.7556, 0.0000, 0.0000,\n        0.0000, 0.0000], device='cuda:0'), Label: -1\nInput: tensor([8.0000, 2.7107, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        4.6987, 0.0000], device='cuda:0'), Label: -1\n500000\n100000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**combine in and out datas**","metadata":{"id":"AbYRXfSdwIdT"}},{"cell_type":"code","source":"private_train_shadow_loader, private_test_shadow_loader = get_shadow_datasets(private_combined_in_data, private_combined_out_data)\n\n# Example: Print sizes to verify\n# print(f\"Total Combined Dataset Size: {len(combined_dataset)}\")\nprint(f\"Training Data Size: {len(private_train_shadow_loader)}\")\nprint(f\"Test Data Size: {len(private_test_shadow_loader)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-OnJpp3bakY","outputId":"0b701899-3ed5-4d77-a441-923d1ff73fea","execution":{"iopub.status.busy":"2024-07-02T23:00:05.509457Z","iopub.execute_input":"2024-07-02T23:00:05.510226Z","iopub.status.idle":"2024-07-02T23:00:06.352585Z","shell.execute_reply.started":"2024-07-02T23:00:05.510190Z","shell.execute_reply":"2024-07-02T23:00:06.351349Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Training Data Size: 750\nTest Data Size: 188\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**private attacker model sample train data**","metadata":{"id":"sHqAxZzswaBR"}},{"cell_type":"code","source":"print(f'sample train data: {private_train_shadow_loader.dataset[0]}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jb_I_AzqcWGt","outputId":"610090ef-bb4d-46f8-e8c6-727bffaf532d","execution":{"iopub.status.busy":"2024-07-02T17:30:38.888703Z","iopub.execute_input":"2024-07-02T17:30:38.888990Z","iopub.status.idle":"2024-07-02T17:30:38.895256Z","shell.execute_reply.started":"2024-07-02T17:30:38.888964Z","shell.execute_reply":"2024-07-02T17:30:38.894307Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"sample train data: (tensor([9.0000, 0.0000, 1.1647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 1.7250], device='cuda:0'), -1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Seperate parts of data for linear regression model**","metadata":{"id":"iFC03daHwcaF"}},{"cell_type":"code","source":"# Extract data and labels from DataLoader\nprivate_X_train, private_y_train = extract_data_and_labels(private_train_shadow_loader)\nprivate_X_test, private_y_test = extract_data_and_labels(private_test_shadow_loader)","metadata":{"id":"Z_DZmztOcaFi","execution":{"iopub.status.busy":"2024-07-02T23:00:06.354285Z","iopub.execute_input":"2024-07-02T23:00:06.354599Z","iopub.status.idle":"2024-07-02T23:00:06.694616Z","shell.execute_reply.started":"2024-07-02T23:00:06.354574Z","shell.execute_reply":"2024-07-02T23:00:06.693480Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**Train linear regression attacker model for private dataset**","metadata":{"id":"-iXZXtEBwjba"}},{"cell_type":"code","source":"lr = LR()\nlr.fit(private_X_train, private_y_train)\n# Evaluate the model\nprivate_y_pred = lr.predict(private_X_test)\nwith open('advanced_private_attack_model.pkl', 'wb') as file:\n    pickle.dump(lr, file)\n\n# # Create an instance of the SVM classifier with a linear kernel\n# clf = SVC(kernel='linear', C=10)\n\n# # Train the SVM classifier\n# clf.fit(X_train, y_train)\n\n# # Make predictions with the trained model\n# predictions = clf.predict(X_test)","metadata":{"id":"5fcvDuSeciwK","execution":{"iopub.status.busy":"2024-07-02T23:00:06.697024Z","iopub.execute_input":"2024-07-02T23:00:06.697449Z","iopub.status.idle":"2024-07-02T23:00:06.819349Z","shell.execute_reply.started":"2024-07-02T23:00:06.697414Z","shell.execute_reply":"2024-07-02T23:00:06.817729Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"**check accuracy of private attacker model**","metadata":{"id":"QHGp7CoawoXD"}},{"cell_type":"code","source":"accuracy = accuracy_score(private_y_test, private_y_pred)\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n# accuracy = accuracy_score(y_test, predictions)\n# print(\"Accuracy:\", accuracy)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vdgUcNRzwo28","outputId":"9dbcb4eb-88bd-48c6-b9d9-0377ffa44ef9","execution":{"iopub.status.busy":"2024-07-02T23:00:06.828638Z","iopub.execute_input":"2024-07-02T23:00:06.834326Z","iopub.status.idle":"2024-07-02T23:00:06.854420Z","shell.execute_reply.started":"2024-07-02T23:00:06.834260Z","shell.execute_reply":"2024-07-02T23:00:06.852228Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Accuracy: 82.88%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***3 Membership Inference Attack***\n\n**Simulation Question 8.**\n\n**Attempt to train an attacker model for the given private model\n(private_model.pth). We will test it on our dataset during the online presentation session. A\ncompetitive bonus point is available for the best performance.**","metadata":{}},{"cell_type":"markdown","source":"**Given code**","metadata":{}},{"cell_type":"code","source":"from torchvision import models\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass CIFAR10Classifier(nn.Module):\n  def __init__(self):\n    super(CIFAR10Classifier, self).__init__()\n    self.conv1 = nn.Conv2d(3, 16, 3, 1)\n    self.conv2 = nn.Conv2d(16, 32, 3, 1)\n    self.dropout1 = nn.Dropout2d(0.25)\n    self.dropout2 = nn.Dropout2d(0.5)\n    self.fc1 = nn.Linear(6272, 64)\n    self.fc2 = nn.Linear(64, 10)\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = F.relu(x)\n    x = self.conv2(x)\n    x = F.relu(x)\n    x = F.max_pool2d(x, 2)\n    x = self.dropout1(x)\n    x = torch.flatten(x, 1)\n    x = self.fc1(x)\n    x = F.relu(x)\n    x = self.dropout2(x)\n    x = self.fc2(x)\n    return x\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:06:31.449761Z","iopub.execute_input":"2024-07-02T23:06:31.450398Z","iopub.status.idle":"2024-07-02T23:06:31.461105Z","shell.execute_reply.started":"2024-07-02T23:06:31.450364Z","shell.execute_reply":"2024-07-02T23:06:31.459893Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"**Going to complete this cell**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nfrom torch.utils.data import Subset, DataLoader, TensorDataset\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score ,f1_score\nfrom sklearn.linear_model import LogisticRegression\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = CIFAR10Classifier()\nstate_dict = torch.load(\"model_state_dict.pth\", map_location=device)\nnew_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\nmodel.load_state_dict(new_state_dict)\nmodel.to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nDATA_ROOT = '../cifar10'\nBATCH_SIZE = 64\n\n# Load the indices from list.txt\nindices_file = 'list.txt' ############\nwith open(indices_file, 'r') as f:\n    indices = [int(line.strip()) for line in f]\n\nfull_train_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\ntest_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n\ntrain_indices_set = set(indices)\nall_indices = set(range(len(full_train_dataset)))\nother_indices = list(all_indices - train_indices_set)\n\ntrain_dataset = Subset(full_train_dataset, indices[:len(indices)//2])  ###########\nother_dataset = Subset(full_train_dataset, other_indices)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\nother_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Create labels\ntrain_labels = torch.ones(len(train_dataset)).to(device)\nother_labels = torch.zeros(len(other_dataset)).to(device)\ntest_labels = torch.zeros(len(test_dataset)).to(device)\n####################################\n#if you have an attacker model for each class, modify the above code.\n####################################\n\ndef extract_features(model, dataloader):\n    model.eval()\n    features = []\n    with torch.no_grad():\n        for data in dataloader:\n            inputs, _ = data\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            features.append(outputs)\n    return torch.cat(features).to(device)\n\ntrain_features = extract_features(model, train_loader)\nother_features = extract_features(model, other_loader)\ntest_features = extract_features(model, test_loader)\n\n\ncombined_features = torch.cat((train_features, other_features, test_features))\ncombined_labels = torch.cat((train_labels, other_labels, test_labels))\n\n\nnew_dataset = TensorDataset(combined_features, combined_labels)\nnew_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n#load your attacker model\n#############################################\n# attackers created in question 6\nwith open('basic_attack_model.pkl', 'rb') as file:\n    basic_attack_model = pickle.load(file)\nwith open('basic_private_attack_model.pkl', 'rb') as file:\n    basic_private_attack_model = pickle.load(file)\n# attackers created in question 7\nwith open('advanced_attack_model.pkl', 'rb') as file:\n    advanced_attack_model = pickle.load(file)\nwith open('advanced_private_attack_model.pkl', 'rb') as file:\n    advanced_private_attack_model = pickle.load(file)    \n\n# Calculate training accuracy, confusion matrix, precision, and recall\nbinary_classifier.eval()\nall_labels = []\nall_predicted = []\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for features, labels in new_loader:\n        features, labels = features.to(device), labels.to(device)\n        outputs = attacker(features).squeeze()\n        predicted = (outputs > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        all_labels.extend(labels.cpu().numpy())\n        all_predicted.extend(predicted.cpu().numpy())\n\naccuracy = correct / total\nprint(f'Training Accuracy: {accuracy:.4f}')\n\ncm = confusion_matrix(all_labels, all_predicted)\nprecision = precision_score(all_labels, all_predicted)\nrecall = recall_score(all_labels, all_predicted)\nf1 = f1_score(all_labels, all_predicted)\n\nprint(f'Confusion Matrix:\\n{cm}')\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{},"execution_count":null,"outputs":[]}]}